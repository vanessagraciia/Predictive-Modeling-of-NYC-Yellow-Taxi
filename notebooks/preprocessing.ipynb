{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/20 23:03:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/20 23:03:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('pulocationid', IntegerType(), True), StructField('dolocationid', IntegerType(), True), StructField('fare_amount', DoubleType(), True)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' This code is to read the most updated schema from the taxi data and apply it to \n",
    "other data used '''\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# read 2023-02 (updated schema type) data\n",
    "sdf_feb = spark.read.parquet('../data/landing/taxi_data/2023-02.parquet')\n",
    "sdf_feb = sdf_feb.select('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count'\n",
    ", 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount')\n",
    "\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in sdf_feb.columns]\n",
    "sdf_feb = sdf_feb.select(*consistent_col_casing)\n",
    "\n",
    "# this will be used in the cell below when reading in\n",
    "sdf_schema = sdf_feb.schema\n",
    "sdf_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting month 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 1 done\n",
      "starting month 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 2 done\n",
      "starting month 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 3 done\n",
      "starting month 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 4 done\n",
      "starting month 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 5 done\n",
      "starting month 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 6 done\n",
      "starting month 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 7 done\n",
      "starting month 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 8 done\n",
      "starting month 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 9 done\n",
      "starting month 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 10 done\n",
      "starting month 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 11 done\n",
      "starting month 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 12 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "''' This code is to change the schema to the correct schema and move it to the \n",
    "raw data directory'''\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f\"starting month {month}\")\n",
    "    input_path = f'../data/landing/taxi_data/2022-{str(month).zfill(2)}.parquet'\n",
    "    output_path = f'../data/raw/2022-{str(month).zfill(2)}.parquet'\n",
    "\n",
    "    sdf_malformed = spark.read.parquet(input_path)\n",
    "    sdf_malformed = sdf_malformed.select('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count'\n",
    ", 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount')\n",
    "        \n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema\n",
    "    sdf_malformed = sdf_malformed.select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)])\n",
    "    sdf_malformed = sdf_malformed.coalesce(1)\n",
    "\n",
    "    # write it to raw data directory\n",
    "    sdf_malformed.write.mode('overwrite').parquet(output_path)\n",
    "    print(f\"month {month} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>pulocationid</th><th>dolocationid</th><th>fare_amount</th></tr>\n",
       "<tr><td>2022-10-01 00:03:41</td><td>2022-10-01 00:18:39</td><td>1</td><td>1.7</td><td>249</td><td>107</td><td>9.5</td></tr>\n",
       "<tr><td>2022-10-01 00:14:30</td><td>2022-10-01 00:19:48</td><td>2</td><td>0.72</td><td>151</td><td>238</td><td>5.5</td></tr>\n",
       "<tr><td>2022-10-01 00:27:13</td><td>2022-10-01 00:37:41</td><td>1</td><td>1.74</td><td>238</td><td>166</td><td>9.0</td></tr>\n",
       "<tr><td>2022-10-01 00:32:53</td><td>2022-10-01 00:38:55</td><td>0</td><td>1.3</td><td>142</td><td>239</td><td>6.5</td></tr>\n",
       "<tr><td>2022-10-01 00:44:55</td><td>2022-10-01 00:50:21</td><td>0</td><td>1.0</td><td>238</td><td>166</td><td>6.0</td></tr>\n",
       "<tr><td>2022-10-01 00:22:52</td><td>2022-10-01 00:52:14</td><td>1</td><td>6.8</td><td>186</td><td>41</td><td>25.5</td></tr>\n",
       "<tr><td>2022-10-01 00:33:19</td><td>2022-10-01 00:44:51</td><td>3</td><td>1.88</td><td>162</td><td>145</td><td>10.5</td></tr>\n",
       "<tr><td>2022-10-01 00:02:42</td><td>2022-10-01 00:50:01</td><td>1</td><td>12.2</td><td>100</td><td>22</td><td>41.0</td></tr>\n",
       "<tr><td>2022-10-01 00:06:35</td><td>2022-10-01 00:24:38</td><td>1</td><td>7.79</td><td>138</td><td>112</td><td>23.5</td></tr>\n",
       "<tr><td>2022-10-01 00:29:25</td><td>2022-10-01 00:43:15</td><td>1</td><td>4.72</td><td>145</td><td>75</td><td>14.5</td></tr>\n",
       "<tr><td>2022-10-01 00:01:55</td><td>2022-10-01 00:20:16</td><td>1</td><td>8.8</td><td>138</td><td>236</td><td>26.0</td></tr>\n",
       "<tr><td>2022-10-01 00:27:48</td><td>2022-10-01 00:59:50</td><td>1</td><td>8.6</td><td>140</td><td>36</td><td>29.5</td></tr>\n",
       "<tr><td>2022-10-01 00:05:27</td><td>2022-10-01 00:35:33</td><td>4</td><td>12.0</td><td>70</td><td>230</td><td>36.5</td></tr>\n",
       "<tr><td>2022-10-01 00:38:53</td><td>2022-10-01 00:48:13</td><td>2</td><td>1.4</td><td>230</td><td>68</td><td>8.5</td></tr>\n",
       "<tr><td>2022-10-01 00:24:40</td><td>2022-10-01 00:30:23</td><td>1</td><td>0.76</td><td>79</td><td>113</td><td>5.5</td></tr>\n",
       "<tr><td>2022-10-01 00:32:22</td><td>2022-10-01 00:58:55</td><td>1</td><td>7.8</td><td>113</td><td>116</td><td>26.5</td></tr>\n",
       "<tr><td>2022-10-01 00:17:08</td><td>2022-10-01 00:30:50</td><td>2</td><td>2.9</td><td>13</td><td>249</td><td>12.0</td></tr>\n",
       "<tr><td>2022-10-01 00:32:14</td><td>2022-10-01 00:44:35</td><td>1</td><td>1.71</td><td>249</td><td>79</td><td>9.0</td></tr>\n",
       "<tr><td>2022-10-01 00:09:24</td><td>2022-10-01 00:21:45</td><td>1</td><td>2.3</td><td>48</td><td>249</td><td>10.0</td></tr>\n",
       "<tr><td>2022-10-01 00:22:29</td><td>2022-10-01 00:33:53</td><td>2</td><td>1.67</td><td>249</td><td>224</td><td>8.5</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+\n",
       "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pulocationid|dolocationid|fare_amount|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+\n",
       "| 2022-10-01 00:03:41|  2022-10-01 00:18:39|              1|          1.7|         249|         107|        9.5|\n",
       "| 2022-10-01 00:14:30|  2022-10-01 00:19:48|              2|         0.72|         151|         238|        5.5|\n",
       "| 2022-10-01 00:27:13|  2022-10-01 00:37:41|              1|         1.74|         238|         166|        9.0|\n",
       "| 2022-10-01 00:32:53|  2022-10-01 00:38:55|              0|          1.3|         142|         239|        6.5|\n",
       "| 2022-10-01 00:44:55|  2022-10-01 00:50:21|              0|          1.0|         238|         166|        6.0|\n",
       "| 2022-10-01 00:22:52|  2022-10-01 00:52:14|              1|          6.8|         186|          41|       25.5|\n",
       "| 2022-10-01 00:33:19|  2022-10-01 00:44:51|              3|         1.88|         162|         145|       10.5|\n",
       "| 2022-10-01 00:02:42|  2022-10-01 00:50:01|              1|         12.2|         100|          22|       41.0|\n",
       "| 2022-10-01 00:06:35|  2022-10-01 00:24:38|              1|         7.79|         138|         112|       23.5|\n",
       "| 2022-10-01 00:29:25|  2022-10-01 00:43:15|              1|         4.72|         145|          75|       14.5|\n",
       "| 2022-10-01 00:01:55|  2022-10-01 00:20:16|              1|          8.8|         138|         236|       26.0|\n",
       "| 2022-10-01 00:27:48|  2022-10-01 00:59:50|              1|          8.6|         140|          36|       29.5|\n",
       "| 2022-10-01 00:05:27|  2022-10-01 00:35:33|              4|         12.0|          70|         230|       36.5|\n",
       "| 2022-10-01 00:38:53|  2022-10-01 00:48:13|              2|          1.4|         230|          68|        8.5|\n",
       "| 2022-10-01 00:24:40|  2022-10-01 00:30:23|              1|         0.76|          79|         113|        5.5|\n",
       "| 2022-10-01 00:32:22|  2022-10-01 00:58:55|              1|          7.8|         113|         116|       26.5|\n",
       "| 2022-10-01 00:17:08|  2022-10-01 00:30:50|              2|          2.9|          13|         249|       12.0|\n",
       "| 2022-10-01 00:32:14|  2022-10-01 00:44:35|              1|         1.71|         249|          79|        9.0|\n",
       "| 2022-10-01 00:09:24|  2022-10-01 00:21:45|              1|          2.3|          48|         249|       10.0|\n",
       "| 2022-10-01 00:22:29|  2022-10-01 00:33:53|              2|         1.67|         249|         224|        8.5|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data \n",
    "sdf = spark.read.schema(sdf_schema).parquet('../data/raw/*')\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39656098"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56955"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering the data\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "from datetime import datetime\n",
    "\n",
    "start_date = datetime.strptime(\"2022-01-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "end_date = datetime.strptime(\"2022-12-31 23:59:59\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "filtered_df = sdf.where(\n",
    "    (year(F.col('tpep_pickup_datetime')) == 2022)\n",
    "    & (year(F.col('tpep_dropoff_datetime')) == 2022)\n",
    "    & (F.col('tpep_pickup_datetime') >= start_date) \n",
    "    & (F.col('tpep_pickup_datetime') <= end_date)\n",
    "    & (F.col('tpep_dropoff_datetime') >= start_date) \n",
    "    & (F.col('tpep_dropoff_datetime') <= end_date)\n",
    ")\n",
    "sdf.count() - filtered_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32021"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df2 = filtered_df.where(\n",
    "(F.col('tpep_dropoff_datetime') > F.col('tpep_pickup_datetime'))\n",
    ")\n",
    "filtered_df.count() - filtered_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calculate outlier\n",
    "percentile_99_distance = sdf.approxQuantile(\"trip_distance\", [0.99], 0.001)[0]\n",
    "percentile_1_distance = sdf.approxQuantile(\"trip_distance\", [0.01], 0.001)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more data filtering \n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf_filtered = filtered_df2.where(\n",
    "    (F.col('passenger_count') > 0)\n",
    "    & (F.col('trip_distance') < percentile_99_distance)\n",
    "    & (F.col('trip_distance') > percentile_1_distance)\n",
    "    & (F.col('fare_amount') > 3.0)\n",
    "    & (F.col('pulocationid').between(1, 263))\n",
    "    & (F.col('dolocationid').between(1, 263))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3888647"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data count check\n",
    "sdf_filtered.count() - filtered_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>pulocationid</th><th>dolocationid</th><th>fare_amount</th><th>pickup_day</th></tr>\n",
       "<tr><td>2022-10-01 00:03:41</td><td>2022-10-01 00:18:39</td><td>1</td><td>1.7</td><td>249</td><td>107</td><td>9.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:14:30</td><td>2022-10-01 00:19:48</td><td>2</td><td>0.72</td><td>151</td><td>238</td><td>5.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:27:13</td><td>2022-10-01 00:37:41</td><td>1</td><td>1.74</td><td>238</td><td>166</td><td>9.0</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:22:52</td><td>2022-10-01 00:52:14</td><td>1</td><td>6.8</td><td>186</td><td>41</td><td>25.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:33:19</td><td>2022-10-01 00:44:51</td><td>3</td><td>1.88</td><td>162</td><td>145</td><td>10.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:02:42</td><td>2022-10-01 00:50:01</td><td>1</td><td>12.2</td><td>100</td><td>22</td><td>41.0</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:06:35</td><td>2022-10-01 00:24:38</td><td>1</td><td>7.79</td><td>138</td><td>112</td><td>23.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:29:25</td><td>2022-10-01 00:43:15</td><td>1</td><td>4.72</td><td>145</td><td>75</td><td>14.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:01:55</td><td>2022-10-01 00:20:16</td><td>1</td><td>8.8</td><td>138</td><td>236</td><td>26.0</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:27:48</td><td>2022-10-01 00:59:50</td><td>1</td><td>8.6</td><td>140</td><td>36</td><td>29.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:05:27</td><td>2022-10-01 00:35:33</td><td>4</td><td>12.0</td><td>70</td><td>230</td><td>36.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:38:53</td><td>2022-10-01 00:48:13</td><td>2</td><td>1.4</td><td>230</td><td>68</td><td>8.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:24:40</td><td>2022-10-01 00:30:23</td><td>1</td><td>0.76</td><td>79</td><td>113</td><td>5.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:32:22</td><td>2022-10-01 00:58:55</td><td>1</td><td>7.8</td><td>113</td><td>116</td><td>26.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:17:08</td><td>2022-10-01 00:30:50</td><td>2</td><td>2.9</td><td>13</td><td>249</td><td>12.0</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:32:14</td><td>2022-10-01 00:44:35</td><td>1</td><td>1.71</td><td>249</td><td>79</td><td>9.0</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:09:24</td><td>2022-10-01 00:21:45</td><td>1</td><td>2.3</td><td>48</td><td>249</td><td>10.0</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:22:29</td><td>2022-10-01 00:33:53</td><td>2</td><td>1.67</td><td>249</td><td>224</td><td>8.5</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:37:17</td><td>2022-10-01 00:48:20</td><td>1</td><td>1.72</td><td>224</td><td>114</td><td>9.0</td><td>Sat</td></tr>\n",
       "<tr><td>2022-10-01 00:49:35</td><td>2022-10-01 00:59:38</td><td>2</td><td>1.33</td><td>114</td><td>79</td><td>8.0</td><td>Sat</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
       "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pulocationid|dolocationid|fare_amount|pickup_day|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
       "| 2022-10-01 00:03:41|  2022-10-01 00:18:39|              1|          1.7|         249|         107|        9.5|       Sat|\n",
       "| 2022-10-01 00:14:30|  2022-10-01 00:19:48|              2|         0.72|         151|         238|        5.5|       Sat|\n",
       "| 2022-10-01 00:27:13|  2022-10-01 00:37:41|              1|         1.74|         238|         166|        9.0|       Sat|\n",
       "| 2022-10-01 00:22:52|  2022-10-01 00:52:14|              1|          6.8|         186|          41|       25.5|       Sat|\n",
       "| 2022-10-01 00:33:19|  2022-10-01 00:44:51|              3|         1.88|         162|         145|       10.5|       Sat|\n",
       "| 2022-10-01 00:02:42|  2022-10-01 00:50:01|              1|         12.2|         100|          22|       41.0|       Sat|\n",
       "| 2022-10-01 00:06:35|  2022-10-01 00:24:38|              1|         7.79|         138|         112|       23.5|       Sat|\n",
       "| 2022-10-01 00:29:25|  2022-10-01 00:43:15|              1|         4.72|         145|          75|       14.5|       Sat|\n",
       "| 2022-10-01 00:01:55|  2022-10-01 00:20:16|              1|          8.8|         138|         236|       26.0|       Sat|\n",
       "| 2022-10-01 00:27:48|  2022-10-01 00:59:50|              1|          8.6|         140|          36|       29.5|       Sat|\n",
       "| 2022-10-01 00:05:27|  2022-10-01 00:35:33|              4|         12.0|          70|         230|       36.5|       Sat|\n",
       "| 2022-10-01 00:38:53|  2022-10-01 00:48:13|              2|          1.4|         230|          68|        8.5|       Sat|\n",
       "| 2022-10-01 00:24:40|  2022-10-01 00:30:23|              1|         0.76|          79|         113|        5.5|       Sat|\n",
       "| 2022-10-01 00:32:22|  2022-10-01 00:58:55|              1|          7.8|         113|         116|       26.5|       Sat|\n",
       "| 2022-10-01 00:17:08|  2022-10-01 00:30:50|              2|          2.9|          13|         249|       12.0|       Sat|\n",
       "| 2022-10-01 00:32:14|  2022-10-01 00:44:35|              1|         1.71|         249|          79|        9.0|       Sat|\n",
       "| 2022-10-01 00:09:24|  2022-10-01 00:21:45|              1|          2.3|          48|         249|       10.0|       Sat|\n",
       "| 2022-10-01 00:22:29|  2022-10-01 00:33:53|              2|         1.67|         249|         224|        8.5|       Sat|\n",
       "| 2022-10-01 00:37:17|  2022-10-01 00:48:20|              1|         1.72|         224|         114|        9.0|       Sat|\n",
       "| 2022-10-01 00:49:35|  2022-10-01 00:59:38|              2|         1.33|         114|          79|        8.0|       Sat|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract day from datetime \n",
    "from pyspark.sql.functions import date_format, unix_timestamp\n",
    "\n",
    "sdf_filtered = sdf_filtered.withColumn('pickup_day', date_format('tpep_pickup_datetime', 'EE'))\n",
    "sdf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the trip duration \n",
    "sdf_day = sdf_filtered.withColumn('trip_duration_in_sec', \n",
    "               unix_timestamp('tpep_dropoff_datetime') - unix_timestamp('tpep_pickup_datetime'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for trip duration longer than one minute\n",
    "sdf_day = sdf_day.where(\n",
    "    (F.col('trip_duration_in_sec') > 60)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54262"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_filtered.count() - sdf_day.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>datetime</th><th>temp</th><th>precip</th><th>snow</th><th>windspeed</th><th>conditions</th></tr>\n",
       "<tr><td>2021-12-31 00:00:00</td><td>9.4</td><td>0.0</td><td>0.0</td><td>3.5</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 01:00:00</td><td>9.0</td><td>0.0</td><td>0.0</td><td>7.4</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 02:00:00</td><td>9.4</td><td>0.0</td><td>0.0</td><td>3.0</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 03:00:00</td><td>9.4</td><td>0.0</td><td>0.0</td><td>6.5</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 04:00:00</td><td>9.0</td><td>0.0</td><td>0.0</td><td>3.5</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 05:00:00</td><td>9.2</td><td>0.0</td><td>0.0</td><td>2.5</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 06:00:00</td><td>9.4</td><td>0.0</td><td>0.0</td><td>0.0</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 07:00:00</td><td>10.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 08:00:00</td><td>10.2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 09:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>0.0</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 10:00:00</td><td>10.8</td><td>0.0</td><td>0.0</td><td>3.5</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 11:00:00</td><td>11.8</td><td>0.0</td><td>0.0</td><td>7.9</td><td>Clear</td></tr>\n",
       "<tr><td>2021-12-31 12:00:00</td><td>11.8</td><td>0.0</td><td>0.0</td><td>7.4</td><td>Partially cloudy</td></tr>\n",
       "<tr><td>2021-12-31 13:00:00</td><td>11.3</td><td>0.0</td><td>0.0</td><td>11.2</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 14:00:00</td><td>11.3</td><td>0.0</td><td>0.0</td><td>4.4</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 15:00:00</td><td>11.3</td><td>0.0</td><td>0.0</td><td>7.9</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 16:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>12.1</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 17:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>10.3</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 18:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>3.5</td><td>Overcast</td></tr>\n",
       "<tr><td>2021-12-31 19:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>10.0</td><td>Overcast</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------------------+----+------+----+---------+----------------+\n",
       "|           datetime|temp|precip|snow|windspeed|      conditions|\n",
       "+-------------------+----+------+----+---------+----------------+\n",
       "|2021-12-31 00:00:00| 9.4|   0.0| 0.0|      3.5|        Overcast|\n",
       "|2021-12-31 01:00:00| 9.0|   0.0| 0.0|      7.4|        Overcast|\n",
       "|2021-12-31 02:00:00| 9.4|   0.0| 0.0|      3.0|        Overcast|\n",
       "|2021-12-31 03:00:00| 9.4|   0.0| 0.0|      6.5|        Overcast|\n",
       "|2021-12-31 04:00:00| 9.0|   0.0| 0.0|      3.5|        Overcast|\n",
       "|2021-12-31 05:00:00| 9.2|   0.0| 0.0|      2.5|        Overcast|\n",
       "|2021-12-31 06:00:00| 9.4|   0.0| 0.0|      0.0|        Overcast|\n",
       "|2021-12-31 07:00:00|10.0|   0.0| 0.0|      0.0|        Overcast|\n",
       "|2021-12-31 08:00:00|10.2|   0.0| 0.0|      0.0|        Overcast|\n",
       "|2021-12-31 09:00:00|10.7|   0.0| 0.0|      0.0|        Overcast|\n",
       "|2021-12-31 10:00:00|10.8|   0.0| 0.0|      3.5|        Overcast|\n",
       "|2021-12-31 11:00:00|11.8|   0.0| 0.0|      7.9|           Clear|\n",
       "|2021-12-31 12:00:00|11.8|   0.0| 0.0|      7.4|Partially cloudy|\n",
       "|2021-12-31 13:00:00|11.3|   0.0| 0.0|     11.2|        Overcast|\n",
       "|2021-12-31 14:00:00|11.3|   0.0| 0.0|      4.4|        Overcast|\n",
       "|2021-12-31 15:00:00|11.3|   0.0| 0.0|      7.9|        Overcast|\n",
       "|2021-12-31 16:00:00|10.7|   0.0| 0.0|     12.1|        Overcast|\n",
       "|2021-12-31 17:00:00|10.7|   0.0| 0.0|     10.3|        Overcast|\n",
       "|2021-12-31 18:00:00|10.7|   0.0| 0.0|      3.5|        Overcast|\n",
       "|2021-12-31 19:00:00|10.7|   0.0| 0.0|     10.0|        Overcast|\n",
       "+-------------------+----+------+----+---------+----------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read weather data\n",
    "weather = spark.read.csv(\"../data/landing/weather/2022.csv\", header=True, inferSchema=True)\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change weather timezone to UTC timezone\n",
    "from pyspark.sql.functions import to_utc_timestamp\n",
    "weather = weather.withColumn('utc_datetime', to_utc_timestamp(F.col('datetime'), \"EST\").alias('utc_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>datetime</th><th>temp</th><th>precip</th><th>snow</th><th>windspeed</th><th>conditions</th><th>utc_datetime</th></tr>\n",
       "<tr><td>2021-12-31 00:00:00</td><td>9.4</td><td>0.0</td><td>0.0</td><td>3.5</td><td>Overcast</td><td>2021-12-31 05:00:00</td></tr>\n",
       "<tr><td>2021-12-31 01:00:00</td><td>9.0</td><td>0.0</td><td>0.0</td><td>7.4</td><td>Overcast</td><td>2021-12-31 06:00:00</td></tr>\n",
       "<tr><td>2021-12-31 02:00:00</td><td>9.4</td><td>0.0</td><td>0.0</td><td>3.0</td><td>Overcast</td><td>2021-12-31 07:00:00</td></tr>\n",
       "<tr><td>2021-12-31 03:00:00</td><td>9.4</td><td>0.0</td><td>0.0</td><td>6.5</td><td>Overcast</td><td>2021-12-31 08:00:00</td></tr>\n",
       "<tr><td>2021-12-31 04:00:00</td><td>9.0</td><td>0.0</td><td>0.0</td><td>3.5</td><td>Overcast</td><td>2021-12-31 09:00:00</td></tr>\n",
       "<tr><td>2021-12-31 05:00:00</td><td>9.2</td><td>0.0</td><td>0.0</td><td>2.5</td><td>Overcast</td><td>2021-12-31 10:00:00</td></tr>\n",
       "<tr><td>2021-12-31 06:00:00</td><td>9.4</td><td>0.0</td><td>0.0</td><td>0.0</td><td>Overcast</td><td>2021-12-31 11:00:00</td></tr>\n",
       "<tr><td>2021-12-31 07:00:00</td><td>10.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>Overcast</td><td>2021-12-31 12:00:00</td></tr>\n",
       "<tr><td>2021-12-31 08:00:00</td><td>10.2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>Overcast</td><td>2021-12-31 13:00:00</td></tr>\n",
       "<tr><td>2021-12-31 09:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>0.0</td><td>Overcast</td><td>2021-12-31 14:00:00</td></tr>\n",
       "<tr><td>2021-12-31 10:00:00</td><td>10.8</td><td>0.0</td><td>0.0</td><td>3.5</td><td>Overcast</td><td>2021-12-31 15:00:00</td></tr>\n",
       "<tr><td>2021-12-31 11:00:00</td><td>11.8</td><td>0.0</td><td>0.0</td><td>7.9</td><td>Clear</td><td>2021-12-31 16:00:00</td></tr>\n",
       "<tr><td>2021-12-31 12:00:00</td><td>11.8</td><td>0.0</td><td>0.0</td><td>7.4</td><td>Partially cloudy</td><td>2021-12-31 17:00:00</td></tr>\n",
       "<tr><td>2021-12-31 13:00:00</td><td>11.3</td><td>0.0</td><td>0.0</td><td>11.2</td><td>Overcast</td><td>2021-12-31 18:00:00</td></tr>\n",
       "<tr><td>2021-12-31 14:00:00</td><td>11.3</td><td>0.0</td><td>0.0</td><td>4.4</td><td>Overcast</td><td>2021-12-31 19:00:00</td></tr>\n",
       "<tr><td>2021-12-31 15:00:00</td><td>11.3</td><td>0.0</td><td>0.0</td><td>7.9</td><td>Overcast</td><td>2021-12-31 20:00:00</td></tr>\n",
       "<tr><td>2021-12-31 16:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>12.1</td><td>Overcast</td><td>2021-12-31 21:00:00</td></tr>\n",
       "<tr><td>2021-12-31 17:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>10.3</td><td>Overcast</td><td>2021-12-31 22:00:00</td></tr>\n",
       "<tr><td>2021-12-31 18:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>3.5</td><td>Overcast</td><td>2021-12-31 23:00:00</td></tr>\n",
       "<tr><td>2021-12-31 19:00:00</td><td>10.7</td><td>0.0</td><td>0.0</td><td>10.0</td><td>Overcast</td><td>2022-01-01 00:00:00</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------------------+----+------+----+---------+----------------+-------------------+\n",
       "|           datetime|temp|precip|snow|windspeed|      conditions|       utc_datetime|\n",
       "+-------------------+----+------+----+---------+----------------+-------------------+\n",
       "|2021-12-31 00:00:00| 9.4|   0.0| 0.0|      3.5|        Overcast|2021-12-31 05:00:00|\n",
       "|2021-12-31 01:00:00| 9.0|   0.0| 0.0|      7.4|        Overcast|2021-12-31 06:00:00|\n",
       "|2021-12-31 02:00:00| 9.4|   0.0| 0.0|      3.0|        Overcast|2021-12-31 07:00:00|\n",
       "|2021-12-31 03:00:00| 9.4|   0.0| 0.0|      6.5|        Overcast|2021-12-31 08:00:00|\n",
       "|2021-12-31 04:00:00| 9.0|   0.0| 0.0|      3.5|        Overcast|2021-12-31 09:00:00|\n",
       "|2021-12-31 05:00:00| 9.2|   0.0| 0.0|      2.5|        Overcast|2021-12-31 10:00:00|\n",
       "|2021-12-31 06:00:00| 9.4|   0.0| 0.0|      0.0|        Overcast|2021-12-31 11:00:00|\n",
       "|2021-12-31 07:00:00|10.0|   0.0| 0.0|      0.0|        Overcast|2021-12-31 12:00:00|\n",
       "|2021-12-31 08:00:00|10.2|   0.0| 0.0|      0.0|        Overcast|2021-12-31 13:00:00|\n",
       "|2021-12-31 09:00:00|10.7|   0.0| 0.0|      0.0|        Overcast|2021-12-31 14:00:00|\n",
       "|2021-12-31 10:00:00|10.8|   0.0| 0.0|      3.5|        Overcast|2021-12-31 15:00:00|\n",
       "|2021-12-31 11:00:00|11.8|   0.0| 0.0|      7.9|           Clear|2021-12-31 16:00:00|\n",
       "|2021-12-31 12:00:00|11.8|   0.0| 0.0|      7.4|Partially cloudy|2021-12-31 17:00:00|\n",
       "|2021-12-31 13:00:00|11.3|   0.0| 0.0|     11.2|        Overcast|2021-12-31 18:00:00|\n",
       "|2021-12-31 14:00:00|11.3|   0.0| 0.0|      4.4|        Overcast|2021-12-31 19:00:00|\n",
       "|2021-12-31 15:00:00|11.3|   0.0| 0.0|      7.9|        Overcast|2021-12-31 20:00:00|\n",
       "|2021-12-31 16:00:00|10.7|   0.0| 0.0|     12.1|        Overcast|2021-12-31 21:00:00|\n",
       "|2021-12-31 17:00:00|10.7|   0.0| 0.0|     10.3|        Overcast|2021-12-31 22:00:00|\n",
       "|2021-12-31 18:00:00|10.7|   0.0| 0.0|      3.5|        Overcast|2021-12-31 23:00:00|\n",
       "|2021-12-31 19:00:00|10.7|   0.0| 0.0|     10.0|        Overcast|2022-01-01 00:00:00|\n",
       "+-------------------+----+------+----+---------+----------------+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering weather data \n",
    "from pyspark.sql.functions import year\n",
    "weather = weather.where(year(weather[\"utc_datetime\"]) == 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicate\n",
    "duplicate_counts = weather.groupBy('utc_datetime').count()\n",
    "duplicates_count = duplicate_counts.filter(F.col(\"count\") > 1).count()\n",
    "duplicates_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>utc_datetime</th><th>count</th></tr>\n",
       "<tr><td>2022-11-06 06:00:00</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-----+\n",
       "|       utc_datetime|count|\n",
       "+-------------------+-----+\n",
       "|2022-11-06 06:00:00|    2|\n",
       "+-------------------+-----+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data that duplicated\n",
    "duplicate_counts.filter(F.col(\"count\") > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate \n",
    "weather = weather.dropDuplicates(['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8759"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count weather data\n",
    "weather.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outter join the taxi dataset with weather data to check which date is missing in the weather dataset\n",
    "from pyspark.sql.functions import hour, date_trunc\n",
    "\n",
    "combined_df = sdf_day.join(\n",
    "    weather,\n",
    "    (hour(sdf_day[\"tpep_pickup_datetime\"]) == hour(weather[\"utc_datetime\"]))\n",
    "    & (date_trunc(\"day\", sdf_day[\"tpep_pickup_datetime\"]) == date_trunc(\"day\", weather[\"utc_datetime\"])),\n",
    "    \"outer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>pulocationid</th><th>dolocationid</th><th>fare_amount</th><th>pickup_day</th><th>trip_duration_in_sec</th><th>datetime</th><th>temp</th><th>precip</th><th>snow</th><th>windspeed</th><th>conditions</th><th>utc_datetime</th></tr>\n",
       "<tr><td>2022-03-13 07:45:10</td><td>2022-03-13 07:47:22</td><td>5</td><td>0.99</td><td>263</td><td>141</td><td>4.5</td><td>Sun</td><td>132</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:10:52</td><td>2022-03-13 07:47:18</td><td>1</td><td>12.1</td><td>132</td><td>17</td><td>38.0</td><td>Sun</td><td>2186</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:02:48</td><td>2022-03-13 07:32:57</td><td>1</td><td>19.54</td><td>132</td><td>260</td><td>52.0</td><td>Sun</td><td>1809</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:00:55</td><td>2022-03-13 07:20:28</td><td>1</td><td>7.31</td><td>162</td><td>13</td><td>23.5</td><td>Sun</td><td>1173</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:00:47</td><td>2022-03-13 07:08:06</td><td>1</td><td>1.86</td><td>90</td><td>79</td><td>8.0</td><td>Sun</td><td>439</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:13:16</td><td>2022-03-13 07:17:48</td><td>1</td><td>0.96</td><td>260</td><td>260</td><td>5.5</td><td>Sun</td><td>272</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:20:12</td><td>2022-03-13 07:40:11</td><td>1</td><td>4.69</td><td>260</td><td>230</td><td>18.0</td><td>Sun</td><td>1199</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:48:27</td><td>2022-03-13 07:58:20</td><td>1</td><td>1.74</td><td>164</td><td>50</td><td>9.0</td><td>Sun</td><td>593</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:15:34</td><td>2022-03-13 07:25:09</td><td>1</td><td>1.52</td><td>236</td><td>239</td><td>8.0</td><td>Sun</td><td>575</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:42:22</td><td>2022-03-13 07:56:59</td><td>1</td><td>2.86</td><td>68</td><td>231</td><td>13.0</td><td>Sun</td><td>877</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:00:56</td><td>2022-03-13 07:19:49</td><td>6</td><td>5.92</td><td>68</td><td>80</td><td>20.5</td><td>Sun</td><td>1133</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:10:43</td><td>2022-03-13 07:41:10</td><td>1</td><td>17.53</td><td>132</td><td>186</td><td>52.0</td><td>Sun</td><td>1827</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:02:55</td><td>2022-03-13 07:09:04</td><td>1</td><td>1.62</td><td>233</td><td>163</td><td>7.5</td><td>Sun</td><td>369</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:22:52</td><td>2022-03-13 07:38:20</td><td>1</td><td>11.86</td><td>132</td><td>138</td><td>32.0</td><td>Sun</td><td>928</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:32:49</td><td>2022-03-13 07:42:08</td><td>1</td><td>2.1</td><td>238</td><td>140</td><td>9.5</td><td>Sun</td><td>559</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:47:06</td><td>2022-03-13 07:50:29</td><td>1</td><td>0.7</td><td>140</td><td>262</td><td>4.5</td><td>Sun</td><td>203</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:42:22</td><td>2022-03-13 07:47:49</td><td>1</td><td>2.18</td><td>151</td><td>263</td><td>8.0</td><td>Sun</td><td>327</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:49:25</td><td>2022-03-13 08:02:17</td><td>1</td><td>3.5</td><td>141</td><td>90</td><td>13.0</td><td>Sun</td><td>772</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:27:59</td><td>2022-03-13 07:46:33</td><td>2</td><td>12.5</td><td>132</td><td>53</td><td>35.5</td><td>Sun</td><td>1114</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2022-03-13 07:08:01</td><td>2022-03-13 07:13:08</td><td>1</td><td>1.44</td><td>236</td><td>140</td><td>6.5</td><td>Sun</td><td>307</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+--------------------+--------+----+------+----+---------+----------+------------+\n",
       "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pulocationid|dolocationid|fare_amount|pickup_day|trip_duration_in_sec|datetime|temp|precip|snow|windspeed|conditions|utc_datetime|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+--------------------+--------+----+------+----+---------+----------+------------+\n",
       "| 2022-03-13 07:45:10|  2022-03-13 07:47:22|              5|         0.99|         263|         141|        4.5|       Sun|                 132|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:10:52|  2022-03-13 07:47:18|              1|         12.1|         132|          17|       38.0|       Sun|                2186|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:02:48|  2022-03-13 07:32:57|              1|        19.54|         132|         260|       52.0|       Sun|                1809|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:00:55|  2022-03-13 07:20:28|              1|         7.31|         162|          13|       23.5|       Sun|                1173|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:00:47|  2022-03-13 07:08:06|              1|         1.86|          90|          79|        8.0|       Sun|                 439|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:13:16|  2022-03-13 07:17:48|              1|         0.96|         260|         260|        5.5|       Sun|                 272|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:20:12|  2022-03-13 07:40:11|              1|         4.69|         260|         230|       18.0|       Sun|                1199|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:48:27|  2022-03-13 07:58:20|              1|         1.74|         164|          50|        9.0|       Sun|                 593|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:15:34|  2022-03-13 07:25:09|              1|         1.52|         236|         239|        8.0|       Sun|                 575|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:42:22|  2022-03-13 07:56:59|              1|         2.86|          68|         231|       13.0|       Sun|                 877|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:00:56|  2022-03-13 07:19:49|              6|         5.92|          68|          80|       20.5|       Sun|                1133|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:10:43|  2022-03-13 07:41:10|              1|        17.53|         132|         186|       52.0|       Sun|                1827|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:02:55|  2022-03-13 07:09:04|              1|         1.62|         233|         163|        7.5|       Sun|                 369|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:22:52|  2022-03-13 07:38:20|              1|        11.86|         132|         138|       32.0|       Sun|                 928|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:32:49|  2022-03-13 07:42:08|              1|          2.1|         238|         140|        9.5|       Sun|                 559|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:47:06|  2022-03-13 07:50:29|              1|          0.7|         140|         262|        4.5|       Sun|                 203|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:42:22|  2022-03-13 07:47:49|              1|         2.18|         151|         263|        8.0|       Sun|                 327|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:49:25|  2022-03-13 08:02:17|              1|          3.5|         141|          90|       13.0|       Sun|                 772|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:27:59|  2022-03-13 07:46:33|              2|         12.5|         132|          53|       35.5|       Sun|                1114|    null|null|  null|null|     null|      null|        null|\n",
       "| 2022-03-13 07:08:01|  2022-03-13 07:13:08|              1|         1.44|         236|         140|        6.5|       Sun|                 307|    null|null|  null|null|     null|      null|        null|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+--------------------+--------+----+------+----+---------+----------+------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values in weather dataset\n",
    "combined_df.where(F.col('temp').isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>datetime</th><th>temp</th><th>precip</th><th>snow</th><th>windspeed</th><th>conditions</th><th>utc_datetime</th></tr>\n",
       "<tr><td>2022-03-13 01:00:00</td><td>-3.9</td><td>0.0</td><td>0.0</td><td>15.6</td><td>Partially cloudy</td><td>2022-03-13 06:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+----+------+----+---------+----------------+-------------------+\n",
       "|           datetime|temp|precip|snow|windspeed|      conditions|       utc_datetime|\n",
       "+-------------------+----+------+----+---------+----------------+-------------------+\n",
       "|2022-03-13 01:00:00|-3.9|   0.0| 0.0|     15.6|Partially cloudy|2022-03-13 06:00:00|\n",
       "+-------------------+----+------+----+---------+----------------+-------------------+"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the previous hour data\n",
    "weather.where(F.col('utc_datetime') == '2022-03-13 06:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>datetime</th><th>temp</th><th>precip</th><th>snow</th><th>windspeed</th><th>conditions</th><th>utc_datetime</th></tr>\n",
       "<tr><td>2022-03-13 03:00:00</td><td>-3.9</td><td>0.0</td><td>0.0</td><td>14.3</td><td>Partially cloudy</td><td>2022-03-13 08:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+----+------+----+---------+----------------+-------------------+\n",
       "|           datetime|temp|precip|snow|windspeed|      conditions|       utc_datetime|\n",
       "+-------------------+----+------+----+---------+----------------+-------------------+\n",
       "|2022-03-13 03:00:00|-3.9|   0.0| 0.0|     14.3|Partially cloudy|2022-03-13 08:00:00|\n",
       "+-------------------+----+------+----+---------+----------------+-------------------+"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the next hour data\n",
    "weather.where(F.col('utc_datetime') == '2022-03-13 08:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = weather.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new data (row) and add it to weather data\n",
    "missing_data = spark.createDataFrame([('2022-03-13 02:00:00', '-3.9', '0.0', '0.0', f'{(15.6 + 14.3) / 2}', 'Partially cloudy', '2022-03-13 07:00:00')], weather.columns)\n",
    "weather_df = weather.union(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8760"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data count \n",
    "weather_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(39656098, 35624213, 35624213)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inner join the taxi data with completed weather data\n",
    "from pyspark.sql.functions import hour, date_trunc\n",
    "\n",
    "combined_df2 = sdf_day.join(\n",
    "    weather_df,\n",
    "    (hour(sdf_day[\"tpep_pickup_datetime\"]) == hour(weather_df[\"utc_datetime\"]))\n",
    "    & (date_trunc(\"day\", sdf_day[\"tpep_pickup_datetime\"]) == date_trunc(\"day\", weather_df[\"utc_datetime\"])),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "sdf.count(), combined_df2.count(), sdf_day.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'pulocationid',\n",
       " 'dolocationid',\n",
       " 'fare_amount',\n",
       " 'pickup_day',\n",
       " 'trip_duration_in_sec',\n",
       " 'datetime',\n",
       " 'temp',\n",
       " 'precip',\n",
       " 'snow',\n",
       " 'windspeed',\n",
       " 'conditions',\n",
       " 'utc_datetime']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check columns name\n",
    "combined_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unneccessary columns \n",
    "final_df = combined_df2.select(['tpep_pickup_datetime',\n",
    " 'tpep_dropoff_datetime',\n",
    " 'pulocationid',\n",
    " 'dolocationid',\n",
    " 'pickup_day',\n",
    " 'temp',\n",
    " 'precip',\n",
    " 'snow',\n",
    " 'windspeed', \n",
    " 'conditions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((1033269.244 172126.008, 103343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.084341</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((1026308.770 256767.698, 1026495.593 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((992073.467 203714.076, 992068.667 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.092146</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>POLYGON ((935843.310 144283.336, 936046.565 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>259</td>\n",
       "      <td>0.126750</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>Woodlawn/Wakefield</td>\n",
       "      <td>259</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((1025414.782 270986.139, 1025138.624 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>260</td>\n",
       "      <td>0.133514</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>260</td>\n",
       "      <td>Queens</td>\n",
       "      <td>POLYGON ((1011466.966 216463.005, 1011545.889 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>261</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>World Trade Center</td>\n",
       "      <td>261</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((980555.204 196138.486, 980570.792 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>262</td>\n",
       "      <td>0.049064</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>Yorkville East</td>\n",
       "      <td>262</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MULTIPOLYGON (((999804.795 224498.527, 999824....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>263</td>\n",
       "      <td>0.037017</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>263</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((997493.323 220912.386, 997355.264 22...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
       "0           1    0.116357    0.000782           Newark Airport           1   \n",
       "1           2    0.433470    0.004866              Jamaica Bay           2   \n",
       "2           3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
       "3           4    0.043567    0.000112            Alphabet City           4   \n",
       "4           5    0.092146    0.000498            Arden Heights           5   \n",
       "..        ...         ...         ...                      ...         ...   \n",
       "258       259    0.126750    0.000395       Woodlawn/Wakefield         259   \n",
       "259       260    0.133514    0.000422                 Woodside         260   \n",
       "260       261    0.027120    0.000034       World Trade Center         261   \n",
       "261       262    0.049064    0.000122           Yorkville East         262   \n",
       "262       263    0.037017    0.000066           Yorkville West         263   \n",
       "\n",
       "           borough                                           geometry  \n",
       "0              EWR  POLYGON ((933100.918 192536.086, 933091.011 19...  \n",
       "1           Queens  MULTIPOLYGON (((1033269.244 172126.008, 103343...  \n",
       "2            Bronx  POLYGON ((1026308.770 256767.698, 1026495.593 ...  \n",
       "3        Manhattan  POLYGON ((992073.467 203714.076, 992068.667 20...  \n",
       "4    Staten Island  POLYGON ((935843.310 144283.336, 936046.565 14...  \n",
       "..             ...                                                ...  \n",
       "258          Bronx  POLYGON ((1025414.782 270986.139, 1025138.624 ...  \n",
       "259         Queens  POLYGON ((1011466.966 216463.005, 1011545.889 ...  \n",
       "260      Manhattan  POLYGON ((980555.204 196138.486, 980570.792 19...  \n",
       "261      Manhattan  MULTIPOLYGON (((999804.795 224498.527, 999824....  \n",
       "262      Manhattan  POLYGON ((997493.323 220912.386, 997355.264 22...  \n",
       "\n",
       "[263 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the geopandas file \n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "\n",
    "# sf stands for shape file\n",
    "sf = gpd.read_file(\"../data/landing/geopandas/taxi_zones.shp\")\n",
    "zones = pd.read_csv(\"../data/landing/geopandas/taxi_zones.csv\")\n",
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((-74.18445 40.69500, -74.18449 40.695...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.084341</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((-73.84793 40.87134, -73.84725 40.870...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.97177 40.72582, -73.97179 40.725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.092146</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>POLYGON ((-74.17422 40.56257, -74.17349 40.562...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>259</td>\n",
       "      <td>0.126750</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>Woodlawn/Wakefield</td>\n",
       "      <td>259</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((-73.85107 40.91037, -73.85207 40.909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>260</td>\n",
       "      <td>0.133514</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>260</td>\n",
       "      <td>Queens</td>\n",
       "      <td>POLYGON ((-73.90175 40.76078, -73.90147 40.759...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>261</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>World Trade Center</td>\n",
       "      <td>261</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-74.01333 40.70503, -74.01327 40.704...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>262</td>\n",
       "      <td>0.049064</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>Yorkville East</td>\n",
       "      <td>262</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MULTIPOLYGON (((-73.94383 40.78286, -73.94376 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>263</td>\n",
       "      <td>0.037017</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>263</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.95219 40.77302, -73.95269 40.772...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
       "0           1    0.116357    0.000782           Newark Airport           1   \n",
       "1           2    0.433470    0.004866              Jamaica Bay           2   \n",
       "2           3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
       "3           4    0.043567    0.000112            Alphabet City           4   \n",
       "4           5    0.092146    0.000498            Arden Heights           5   \n",
       "..        ...         ...         ...                      ...         ...   \n",
       "258       259    0.126750    0.000395       Woodlawn/Wakefield         259   \n",
       "259       260    0.133514    0.000422                 Woodside         260   \n",
       "260       261    0.027120    0.000034       World Trade Center         261   \n",
       "261       262    0.049064    0.000122           Yorkville East         262   \n",
       "262       263    0.037017    0.000066           Yorkville West         263   \n",
       "\n",
       "           borough                                           geometry  \n",
       "0              EWR  POLYGON ((-74.18445 40.69500, -74.18449 40.695...  \n",
       "1           Queens  MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...  \n",
       "2            Bronx  POLYGON ((-73.84793 40.87134, -73.84725 40.870...  \n",
       "3        Manhattan  POLYGON ((-73.97177 40.72582, -73.97179 40.725...  \n",
       "4    Staten Island  POLYGON ((-74.17422 40.56257, -74.17349 40.562...  \n",
       "..             ...                                                ...  \n",
       "258          Bronx  POLYGON ((-73.85107 40.91037, -73.85207 40.909...  \n",
       "259         Queens  POLYGON ((-73.90175 40.76078, -73.90147 40.759...  \n",
       "260      Manhattan  POLYGON ((-74.01333 40.70503, -74.01327 40.704...  \n",
       "261      Manhattan  MULTIPOLYGON (((-73.94383 40.78286, -73.94376 ...  \n",
       "262      Manhattan  POLYGON ((-73.95219 40.77302, -73.95269 40.772...  \n",
       "\n",
       "[263 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the geometry shaape to to latitude and longitude\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n",
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both files \n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pd.merge(zones, sf, on='LocationID', how='inner')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just take needed column for borough dataframe \n",
    "borough_df = gdf[['LocationID', 'Borough', 'Zone']]\n",
    "borough_df = spark.createDataFrame(borough_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join taxi_data with the borough dataframe\n",
    "taxi_data = final_df.join(\n",
    "    borough_df, final_df[\"pulocationid\"] == borough_df[\"LocationID\"], \"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 23:08:45 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/08/20 23:08:45 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/08/20 23:09:06 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/08/20 23:09:06 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/08/20 23:09:07 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/08/20 23:09:07 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/08/20 23:09:07 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/08/20 23:09:07 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/08/20 23:09:09 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/08/20 23:09:09 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/08/20 23:09:10 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save it to curated directory\n",
    "taxi_data.write.mode('overwrite').parquet('../data/curated/taxi_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
